{"cells":[{"metadata":{},"cell_type":"markdown","source":"Let's begin by importing the necessary packages:"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset\nimport csv, math\nfrom PIL import Image\nimport os\n\n#from utils import CancerDataset\nfrom torch.utils.data import DataLoader\n\n# Input data files are available in the \"../input/\" directory.\nimport os\nprint(os.listdir(\"../input\"))","execution_count":1,"outputs":[{"output_type":"stream","text":"['test', 'sample_submission.csv', 'train_labels.csv', 'train']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Next we will define a CancerDataset, that inherits from the PyTorch \"Dataset\" class. This will be helpful in iterating through our data in batches, shuffling, and augmenting our data. "},{"metadata":{"trusted":true,"_uuid":"c49034fcd756827b3f55e497c3d865be9f11c727"},"cell_type":"code","source":"class CancerDataset(Dataset):\n    \"\"\"Cancer Detection Dataset.\"\"\"\n\n    def __init__(self, df, image_dir, transform=None):\n        \"\"\"\n        Args:\n            csv_file (string): Path to the csv file with annotations.\n            root_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.labels = df.label      \n        self.im_names = df.id\n        self.image_dir = image_dir\n        if transform is not None:\n            self.transform = transform\n        else:\n            self.transform = transforms.Compose([\n                              transforms.ToTensor()])\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        im_name = self.im_names[idx]\n        im_path = os.path.join(self.image_dir, im_name + '.tif')\n        im = Image.open(im_path)\n        im = self.transform(im)\n\n        label = np.array(int(self.labels[idx]))\n\n        sample = {'image': im.type(torch.cuda.FloatTensor),\n                  'label': torch.from_numpy(label).type(torch.cuda.LongTensor)}\n\n        #if self.transform:\n        #    sample = self.transform(sample)\n\n        return sample","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we will define our network structure. For this task, we've chosen 3 layers of convolution and pooling, followed by 3 dense layers."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        # input = 96 x 96 x 3\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, bias=True)\n        self.bn1 = nn.BatchNorm2d(16)\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, bias=True)\n        self.bn2 = nn.BatchNorm2d(32)\n        self.conv3 = nn.Conv2d(32, 64, kernel_size=2, stride=1, bias=True)\n        self.bn3 = nn.BatchNorm2d(64)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.fc1 = nn.Linear(64*10*10, 512)\n        self.fc2 = nn.Linear(512, 256)\n        self.fc3 = nn.Linear(256, 2)\n        #self.sig = nn.Sigmoid()\n        \n    def forward(self, x):\n        # Pytorch is formatted as batch x channels x rows x cols\n        #x = x.permute(0, 3, 1, 2)\n        x = self.conv1(x)\n        x = self.pool(F.relu(self.bn1(x)))\n        x = self.conv2(x)\n        x = self.pool(F.relu(self.bn2(x)))\n        x = self.conv3(x)\n        x = self.pool(F.relu(self.bn3(x)))\n\n        x = x.view(-1, 64*10*10)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\ndef train(model, train_dl, batch_size, num_epochs, learning_rate=0.001):\n\n    # Loss and optimizer\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adamax(model.parameters(), lr=learning_rate)\n    #criterion = nn.BCEWithLogitsLoss()\n    #optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n    # Train the model\n    loss_list = []\n    acc_list = []\n\n    #Print all of the hyperparameters of the training iteration:\n    print(\"===== HYPERPARAMETERS =====\")\n    print(\"batch_size=\", batch_size)\n    print(\"epochs=\", num_epochs)\n    print(\"learning_rate=\", learning_rate)\n    print(\"=\" * 27)\n\n    for epoch in range(num_epochs):\n        for i, data in enumerate(train_dl):\n            im_batch = data['image']\n            label_batch = data['label']\n\n            # Run the forward pass\n            optimizer.zero_grad()\n\n            outputs = model(im_batch)\n            #print(outputs.shape)\n            #print(outputs)\n            \n            loss = criterion(outputs, label_batch)\n            loss_list.append(loss.item())\n\n            # Backprop and perform Adam optimisation\n            loss.backward()\n            optimizer.step()\n\n            # Track the accuracy\n            total = label_batch.size(0)\n            _, predicted = torch.max(outputs.data, 1)\n            correct = (predicted == label_batch).sum().item()\n            acc_list.append(correct / total)\n            if (i + 1) % 200 == 0:\n                print(loss.item())","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We first initialize our datasets and pass them to the dataloaders\n\nThen we configure the device (either GPU or CPU) and initiate training."},{"metadata":{"trusted":true,"_uuid":"8dd43ad5b601a90cf82c00ddf4470004fe3032a8"},"cell_type":"code","source":"traincsv = '../input/train_labels.csv'\nim_dir = '../input/train/'\ntrain_df = pd.read_csv(traincsv)\nim_transform = transforms.Compose([\n                  transforms.RandomHorizontalFlip(), \n                  transforms.RandomVerticalFlip(),\n                  transforms.RandomRotation(20),\n                  transforms.ToTensor()])\n                  #transforms.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5])])\n\nnum_epochs = 3\nbatch_size = 64\n\n# Initialize dataset\nds = CancerDataset(train_df, image_dir=im_dir, transform=im_transform)\n#https://stackoverflow.com/questions/50544730/how-do-i-split-a-custom-dataset-into-training-and-test-datasets\ntrain_size = int(0.8 * len(ds))\nval_size = len(ds) - train_size\ntrain_ds, val_ds = torch.utils.data.random_split(ds, [train_size, val_size])\ntrain_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0)\nval_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True, num_workers=0)\n\n# Device configuration - use GPU if available\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\nmodel = Net().to(device)\n\ntrain(model, train_dl, batch_size, num_epochs, 0.001)","execution_count":4,"outputs":[{"output_type":"stream","text":"===== HYPERPARAMETERS =====\nbatch_size= 64\nepochs= 3\nlearning_rate= 0.001\n===========================\n0.4608437418937683\n0.4939935505390167\n0.26370421051979065\n0.25264474749565125\n0.30566319823265076\n0.3037013113498688\n0.2593081295490265\n0.37768977880477905\n0.2698407769203186\n0.30770060420036316\n0.39942196011543274\n0.37669020891189575\n0.4490520656108856\n0.3043595552444458\n0.28148573637008667\n0.3729715645313263\n0.3450397849082947\n0.3085711896419525\n0.3312935531139374\n0.3430314064025879\n0.24619035422801971\n0.21712902188301086\n0.27644211053848267\n0.5876179337501526\n0.25410646200180054\n0.2204357385635376\n0.283459335565567\n0.28090935945510864\n0.2225927859544754\n0.2801467478275299\n0.4364542067050934\n0.3367641568183899\n0.3021238446235657\n0.29901692271232605\n0.3375479280948639\n0.3891212046146393\n0.3377498388290405\n0.17018640041351318\n0.20743441581726074\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Now that Training has completed, we can evaluate the model. It is important here that we use the command 'torch.no_grad()' to ensure that we no longer update gradients. We are only running inference. "},{"metadata":{"trusted":true,"_uuid":"77525762a4f645c01ae89bd5b9f34f232037add7"},"cell_type":"code","source":"model.eval()  # evaluate using moving mean and variance from batch norm\nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for i, data in enumerate(val_dl):\n        im_batch = data['image']\n        label_batch = data['label']\n\n        # Run the forward pass\n        outputs = model(im_batch)\n        _, predicted = torch.max(outputs.data, 1)\n        total += label_batch.size(0)\n        correct += (predicted == label_batch).sum().item()\n        if i>4:\n            break\n    print('Test Accuracy: {} %'.format(100 * correct / total))\n","execution_count":5,"outputs":[{"output_type":"stream","text":"Test Accuracy: 88.54166666666667 %\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Once we have evaluated our model and we are satisfied, we can generate a dataset and dataloader from the test set."},{"metadata":{"trusted":true,"_uuid":"f57a9d0532a3b01653a8f5cd1dbee48ac615b533"},"cell_type":"code","source":"im_dir = '../input/test/'\ntestcsv = '../input/sample_submission.csv'\ntest_df = pd.read_csv(testcsv)\n\n# Initialize dataset\ntest_ds = CancerDataset(test_df, image_dir=im_dir, transform=None)\ntest_dl = DataLoader(test_ds, batch_size=128, shuffle=False, num_workers=0)","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we run inference on the test set. "},{"metadata":{"trusted":true,"_uuid":"1e9cfc3ce6f2e6587e0c5f5620a3206ca639abbc"},"cell_type":"code","source":"model.eval()  # evaluate using moving mean and variance from batch norm\nwith torch.no_grad():\n    preds = []\n    for i, data in enumerate(test_dl):\n        im_batch = data['image']\n\n        # Run the forward pass\n        outputs = model(im_batch)\n        _, predicted = torch.max(outputs.data, 1)\n        predicted = predicted.detach().cpu().numpy()\n        #predicted = predicted.numpy()\n        for i in predicted:\n            preds.append(i)\n        #print(len(preds))\n\n    print(len(preds))\n\n    ","execution_count":7,"outputs":[{"output_type":"stream","text":"57458\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Save predictions for competition submission!"},{"metadata":{"trusted":true,"_uuid":"4126393dc774a85893d506326b4fa490cd65f8dd"},"cell_type":"code","source":"sub_df = pd.read_csv('../input/sample_submission.csv')\nsub_df.drop('label', axis=1)\nsub_df['label'] = preds\nsub_df.to_csv('submission.csv', index=False)","execution_count":8,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}